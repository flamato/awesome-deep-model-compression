# awesome-deep-model-compression
List of papers related to ML and DL model compression for embedded devices(Mobiles, IoT)

## Machine Learning
- 2017 [Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things](http://proceedings.mlr.press/v70/kumar17a.html)

## Deep Learning
- ### Pruning
  - 1993 [Second order derivatives for network pruning: Optimal brain surgeon](http://papers.nips.cc/paper/647-second-order-derivatives-for-network-pruning-optimal-brain-surgeon.pdf)
  - 2015 NIPS [Learning both weights and connections for efficient neural network](http://papers.nips.cc/paper/5784-learning-both-weights-and-connections-for-efficient-neural-network)
  - 2016 [Pruning filters for efficient convnets](https://arxiv.org/abs/1608.08710)
  - 2016 [Pruning convolutional neural networks for resource efficient transfer learning](https://pdfs.semanticscholar.org/026e/cf916023e13191331a354271b7f9b86e50a1.pdf)
  - 2016 [Pruning Convolutional Neural Networks for Resource Efficient Inference](https://arxiv.org/abs/1611.06440)
  - 2017 [Structured pruning of deep convolutional neural networks.](https://dl.acm.org/citation.cfm?id=3005348)

- ### Quantization

- ### Prunig and Quantization
  - 2015 [Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding](https://arxiv.org/abs/1510.00149)
  
- ### Binarization

- ### Low-Rank-Factorization

- ### Knowledge Distilation

- ### Stucture

- ### Sparse

- ### Others


